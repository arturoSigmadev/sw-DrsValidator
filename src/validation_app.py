#!/usr/bin/env python3
"""
DRS Validation Tool - FastAPI Web Application
Provides web interface for technicians to validate device communications
"""

import os
import sys
import json
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional

from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel

# Determine project root for correct path resolution
project_root = Path(__file__).parent  # This will be the 'src' directory
sys.path.insert(0, str(project_root))

# Import our validation logic
try:
    from validation.tcp_validator import TechnicianTCPValidator, validate_device
    VALIDATION_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import validation logic: {e}")
    try:
        # Fallback to standalone validator
        from validation.standalone_validator import validate_device_standalone as validate_device
        VALIDATION_AVAILABLE = True
        print("Using standalone validator as fallback")
    except ImportError as e2:
        print(f"Warning: Could not import standalone validator: {e2}")
        VALIDATION_AVAILABLE = False

# Import batch commands validator
try:
    from validation.batch_commands_validator import BatchCommandsValidator, CommandType
    BATCH_VALIDATION_AVAILABLE = True
    print("✅ Batch commands validator loaded successfully")
except ImportError as e:
    print(f"Warning: Could not import batch commands validator: {e}")
    BATCH_VALIDATION_AVAILABLE = False

# Alternative simple validation function if imports fail
def simple_validation(device_ip: str, device_type: str, hostname: str = None, live_mode: bool = False):
    """Simple validation function as fallback"""
    import subprocess
    import time
    
    results = {
        "status": "success" if not live_mode else "unknown",
        "device_ip": device_ip,
        "device_type": device_type,
        "hostname": hostname,
        "mode": "live" if live_mode else "mock",
        "timestamp": datetime.now().isoformat(),
        "tests": []
    }
    
    # Simple ping test
    try:
        if live_mode:
            ping_result = subprocess.run(['ping', '-n', '1', device_ip], 
                                       capture_output=True, text=True, timeout=5)
            ping_success = ping_result.returncode == 0
        else:
            ping_success = True  # Mock mode always succeeds
            
        results["tests"].append({
            "name": "ping_test",
            "status": "pass" if ping_success else "fail",
            "message": f"Ping to {device_ip} {'successful' if ping_success else 'failed'}"
        })
    except Exception as e:
        results["tests"].append({
            "name": "ping_test", 
            "status": "fail",
            "message": f"Ping test error: {str(e)}"
        })
    
    return results

# FastAPI app instance
app = FastAPI(
    title="DRS Device Validation Tool",
    description="Web interface for technicians to validate device communications",
    version="1.0.0",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# Static files and templates
# Correctly locate web assets from the project root
app.mount("/static", StaticFiles(directory=Path(__file__).parent / "web/static"), name="static")
templates = Jinja2Templates(directory=Path(__file__).parent / "web/templates")

# Results storage directory - use relative path for development
PROJECT_ROOT = Path(__file__).parent.parent
RESULTS_DIR = PROJECT_ROOT / "results"
RESULTS_DIR.mkdir(exist_ok=True)


# Pydantic models for API
class DeviceConfig(BaseModel):
    ip_address: str
    device_type: str
    device_name: str
    optical_port: Optional[int] = 1
    command: Optional[int] = None


class ValidationThresholds(BaseModel):
    warning_downlink: Optional[int] = 38
    warning_temperature: Optional[int] = 45
    warning_uplink: Optional[int] = 38
    critical_downlink: Optional[int] = 41
    critical_temperature: Optional[int] = 50
    critical_uplink: Optional[int] = 31


class ValidationRequest(BaseModel):
    mode: str  # 'mock' or 'live'
    device_config: DeviceConfig
    thresholds: ValidationThresholds


class ValidationResult(BaseModel):
    status: str  # 'PASS', 'FAIL', 'WARNING'
    message: str
    details: Optional[Dict] = None
    action: Optional[str] = None
    timestamp: datetime


# New Pydantic models for Batch Commands API
class BatchCommandsRequest(BaseModel):
    """Request model for batch commands validation"""
    ip_address: str
    command_type: str  # 'master' or 'remote'
    mode: str = "mock"  # 'mock' or 'live'
    selected_commands: Optional[List[str]] = None
    timeout_seconds: Optional[int] = 3


class BatchCommandResult(BaseModel):
    """Individual command result model"""
    command: str
    command_type: str
    status: str  # 'PASS', 'FAIL', 'TIMEOUT', 'ERROR'
    message: str
    details: Optional[str] = None
    response_data: Optional[str] = None
    decoded_values: Optional[Dict[str, Any]] = None
    duration_ms: int
    error: Optional[str] = None


class BatchCommandsResponse(BaseModel):
    """Response model for batch commands validation"""
    overall_status: str
    command_type: str
    mode: str
    ip_address: str
    total_commands: int
    commands_tested: List[str]
    statistics: Dict[str, Any]
    results: List[Dict[str, Any]]  # List of command results
    duration_ms: int
    timestamp: str


class SupportedCommandsResponse(BaseModel):
    """Response model for supported commands list"""
    master_commands: List[str]
    remote_commands: List[str]
    set_commands: List[str] = []


def save_validation_result(result: Dict[str, Any], request: ValidationRequest) -> str:
    """Save validation result to persistent storage"""
    try:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        device_type = request.device_config.device_type
        ip_address = request.device_config.ip_address.replace(".", "_")
        
        filename = f"{timestamp}_{device_type}_{ip_address}.json"
        filepath = RESULTS_DIR / filename
        
        # Add metadata to result
        result_data = {
            "timestamp": datetime.now().isoformat(),
            "request": {
                "ip_address": request.device_config.ip_address,
                "device_type": request.device_config.device_type,
                "hostname": request.device_config.device_name,
                "live_mode": request.mode == "live"
            },
            "result": result
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False)
        
        logging.info(f"💾 Resultado guardado: {filepath}")
        return str(filepath)
        
    except Exception as e:
        logging.error(f"❌ Error guardando resultado: {e}")
        return ""


# API Endpoints


# Global validation service
class ValidationService:
    """Service that wraps existing test logic for technician use"""
    
    def __init__(self):
        # The direct instantiation of test classes is removed as it's an anti-pattern.
        # This class can be repurposed or removed if not needed.
        pass
    
    async def validate_device(self, request: ValidationRequest) -> ValidationResult:
        """
        Main validation method that uses existing test patterns
        """
        try:
            if request.mode == 'mock':
                result = await self._validate_mock_mode(request)
            elif request.mode == 'live':
                result = await self._validate_live_mode(request)
            else:
                raise ValueError(f"Invalid mode: {request.mode}")
            
            # Save result to persistent storage
            saved_path = save_validation_result(result.dict(), request)
            if saved_path:
                logging.info(f"✅ Validation result saved to {saved_path}")
            
            return result
                
        except Exception as e:
            error_result = ValidationResult(
                status="FAIL",
                message=f"Validation error: {str(e)}",
                action="Check configuration and try again",
                timestamp=datetime.now()
            )
            
            # Save error result too
            save_validation_result(error_result.dict(), request)
            
            return error_result
    
    async def _validate_mock_mode(self, request: ValidationRequest) -> ValidationResult:
        """
        Mock validation using existing TCP transceiver test patterns
        """
        # TODO: Implement using test_tcp_transceiver.py patterns
        return ValidationResult(
            status="PASS",
            message=f"Mock validation successful for {request.device_config.device_type}",
            details={
                "mode": "mock",
                "device": request.device_config.device_type,
                "ip": request.device_config.ip_address
            },
            timestamp=datetime.now()
        )
    
    async def _validate_live_mode(self, request: ValidationRequest) -> ValidationResult:
        """
        Live validation using existing integration test patterns
        """
        # TODO: Implement using test_check_eth_integration.py patterns
        return ValidationResult(
            status="WARNING",
            message=f"Live validation not yet implemented for {request.device_config.device_type}",
            action="Use mock mode for now",
            timestamp=datetime.now()
        )


# Initialize validation service
validation_service = ValidationService()


# API Routes
@app.get("/health")
async def health_check():
    """Health check endpoint for Docker"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}


@app.get("/", response_class=HTMLResponse)
async def root(request: Request):
    """Main technician interface"""
    return templates.TemplateResponse("index.html", {"request": request})


@app.get("/api/test")
async def test_endpoint():
    """Simple test endpoint to verify API is working"""
    return {"status": "success", "message": "API is working", "timestamp": datetime.now().isoformat()}


@app.get("/api/validation/scenarios")
async def get_validation_scenarios():
    """Get available validation scenarios for technicians"""
    return {
        "scenarios": [
            {
                "id": "dmu_basic_check",
                "name": "DMU Basic Communication",
                "description": "Validates basic DMU device communication and monitoring",
                "enabled": True,
                "device_type": "dmu_ethernet",
                "default_ip": "192.168.11.22",
                "default_hostname": "dmu"
            },
            {
                "id": "dru_remote_check", 
                "name": "DRU Remote Device",
                "description": "Validates DRU remote device communication",
                "enabled": True,
                "device_type": "dru_ethernet",
                "default_ip": "192.168.11.100",
                "default_hostname": "dru34132"
            },
            {
                "id": "batch_remote_commands",
                "name": "Batch Remote Commands",
                "description": "Executes batch validation of all remote DRS commands",
                "enabled": True,
                "device_type": "drs_remote",
                "default_ip": "192.168.11.22",
                "default_hostname": "drs-device"
            }
        ]
    }


@app.post("/api/validation/run")
async def run_validation(request: Dict[str, Any]):
    """Execute device validation with current configuration"""
    try:
        # Validate required fields
        required_fields = ["scenario_id", "ip_address", "hostname", "mode"]
        for field in required_fields:
            if field not in request:
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        # Get scenario details to determine device_type
        scenario_id = request["scenario_id"]
        device_type_mapping = {
            "dmu_basic_check": "dmu_ethernet",
            "dru_remote_check": "dru_ethernet", 
            "device_discovery": "discovery_ethernet",
            "batch_remote_commands": "drs_remote"
        }
        
        # Build validation config
        validation_config = {
            "device_type": device_type_mapping.get(scenario_id, "dmu_ethernet"),
            "ip_address": request["ip_address"],
            "hostname": request["hostname"],
            "mode": request["mode"]
        }
        
        # Add additional parameters for DRU
        if scenario_id == "dru_remote_check":
            validation_config["optical_port"] = 1
            validation_config["command"] = 155
        
        # Add thresholds if provided
        if "thresholds" in request:
            validation_config.update(request["thresholds"])
        
        # Execute validation
        if scenario_id == "batch_remote_commands":
            # Special handling for batch remote commands scenario
            if BATCH_VALIDATION_AVAILABLE:
                try:
                    validator = BatchCommandsValidator()
                    batch_result = validator.validate_batch_commands(
                        ip_address=validation_config["ip_address"],
                        command_type=CommandType.REMOTE,
                        mode=validation_config["mode"],
                        selected_commands=None  # None means all remote commands
                    )
                    
                    # Convert batch result to standard validation format
                    # Ensure status values are strings
                    results_list = batch_result["results"]
                    for cmd in results_list:
                        if hasattr(cmd["status"], 'value'):
                            cmd["status"] = cmd["status"].value
                    
                    result = {
                        "overall_status": batch_result["overall_status"],  # Use batch validator's overall status (80% threshold)
                        "message": f"Batch remote commands validation completed. {len([c for c in results_list if c['status'] == 'PASS'])}/{len(results_list)} commands passed.",
                        "mode": validation_config["mode"],
                        "tests": [
                            {
                                "name": f"Remote Command: {cmd['command']}",
                                "status": cmd["status"],
                                "message": cmd.get("message", "Command executed"),
                                "details": f"Response: {cmd.get('response_data', 'N/A')}",
                                "duration_ms": cmd.get("duration_ms", 0)
                            }
                            for cmd in results_list
                        ],
                        "duration_ms": batch_result.get("duration_ms", 0),
                        "timestamp": datetime.now().isoformat(),
                        "command_type": "remote",
                        "total_commands": len(results_list)
                    }
                except Exception as e:
                    result = {
                        "overall_status": "FAIL",
                        "message": f"Batch remote commands validation failed: {str(e)}",
                        "mode": validation_config["mode"],
                        "tests": [
                            {
                                "name": "Batch Remote Commands",
                                "status": "FAIL",
                                "message": f"Validation error: {str(e)}",
                                "details": "Failed to execute remote commands batch"
                            }
                        ],
                        "duration_ms": 0,
                        "timestamp": datetime.now().isoformat()
                    }
            else:
                result = {
                    "overall_status": "FAIL",
                    "message": "Batch commands validator not available",
                    "mode": validation_config["mode"],
                    "tests": [
                        {
                            "name": "Batch Remote Commands",
                            "status": "FAIL",
                            "message": "Batch commands validator not available",
                            "details": "Required dependencies not installed"
                        }
                    ],
                    "duration_ms": 0,
                    "timestamp": datetime.now().isoformat()
                }
        elif VALIDATION_AVAILABLE:
            result = validate_device(validation_config)
        else:
            # Fallback mock result if validation not available
            result = {
                "overall_status": "WARNING",
                "message": "Validation logic not available - returning mock result",
                "mode": validation_config["mode"],
                "tests": [
                    {
                        "name": "Mock Test",
                        "status": "WARNING",
                        "message": "Validation system not fully initialized",
                        "details": "Import dependencies missing"
                    }
                ],
                "duration_ms": 100,
                "timestamp": datetime.now().isoformat()
            }
        
        # Add request context to result
        result["scenario_id"] = scenario_id
        result["mode"] = validation_config["mode"]
        
        return {
            "status": "success",
            "result": result,
            "scenario_id": scenario_id,
            "mode": validation_config["mode"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Validation execution failed: {str(e)}")


@app.post("/api/validation/ping/{ip_address}")
async def ping_device_endpoint(ip_address: str):
    """Test basic network connectivity to device"""
    try:
        if VALIDATION_AVAILABLE:
            validator = TechnicianTCPValidator()
            result = validator.ping_device(ip_address)
            return result
        else:
            # Simple fallback ping using subprocess
            import subprocess
            import platform
            
            ping_cmd = ["ping", "-n", "1"] if platform.system() == "Windows" else ["ping", "-c", "1"]
            ping_cmd.append(ip_address)
            
            try:
                result = subprocess.run(ping_cmd, capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    return {
                        "status": "PASS",
                        "ip_address": ip_address,
                        "message": f"✅ Device at {ip_address} is reachable",
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    return {
                        "status": "FAIL",
                        "ip_address": ip_address,
                        "message": f"❌ Device at {ip_address} is not reachable",
                        "timestamp": datetime.now().isoformat()
                    }
            except subprocess.TimeoutExpired:
                return {
                    "status": "FAIL",
                    "ip_address": ip_address,
                    "message": "❌ Ping timeout - device not responding",
                    "timestamp": datetime.now().isoformat()
                }
                
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ping test failed: {str(e)}")

@app.get("/api/validation/report/{run_id}")
async def get_validation_report(run_id: str):
    """Get detailed validation report"""
    # TODO: Implement report storage and retrieval
    return {
        "run_id": run_id,
        "status": "not_implemented",
        "message": "Report storage system not yet implemented",
        "timestamp": datetime.now().isoformat()
    }


# New Batch Commands Endpoints
@app.post("/api/validation/batch-commands")
async def run_batch_commands(request: BatchCommandsRequest) -> BatchCommandsResponse:
    """
    Execute batch validation of DRS commands using Santone protocol.
    
    Supports both mock and live testing modes with comprehensive
    command validation and SantoneDecoder integration.
    """
    if not BATCH_VALIDATION_AVAILABLE:
        raise HTTPException(
            status_code=503, 
            detail="Batch commands validator not available"
        )
    
    try:
        # Validate command_type parameter
        if request.command_type.lower() not in ['master', 'remote', 'set']:
            raise HTTPException(
                status_code=400,
                detail="command_type must be 'master', 'remote', or 'set'"
            )
        
        # Convert command_type string to enum
        command_type_map = {
            'master': CommandType.MASTER,
            'remote': CommandType.REMOTE,
            'set': CommandType.SET
        }
        command_type = command_type_map[request.command_type.lower()]
        
        # Initialize batch validator
        validator = BatchCommandsValidator()
        
        # Execute batch validation
        result = validator.validate_batch_commands(
            ip_address=request.ip_address,
            command_type=command_type,
            mode=request.mode,
            selected_commands=request.selected_commands
        )
        
        # Return structured response
        return BatchCommandsResponse(
            overall_status=result["overall_status"],
            command_type=result["command_type"],
            mode=result["mode"],
            ip_address=result["ip_address"],
            total_commands=result["total_commands"],
            commands_tested=result["commands_tested"],
            statistics=result["statistics"],
            results=result["results"],
            duration_ms=result["duration_ms"],
            timestamp=result["timestamp"]
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Batch commands validation failed: {str(e)}"
        )


@app.get("/api/validation/supported-commands")
async def get_supported_commands() -> SupportedCommandsResponse:
    """
    Get list of all supported DRS commands for batch validation.
    
    Returns both master and remote commands with decoder mapping information.
    """
    if not BATCH_VALIDATION_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="Batch commands validator not available"
        )
    
    try:
        # Import decoder integration for mapping info
        from validation.decoder_integration import CommandDecoderMapping
        from validation.hex_frames import DRS_MASTER_FRAMES, DRS_REMOTE_FRAMES, DRS_SET_FRAMES
        
        # Get available commands from hex frames
        master_commands = list(DRS_MASTER_FRAMES.keys())
        remote_commands = list(DRS_REMOTE_FRAMES.keys())
        set_commands = list(DRS_SET_FRAMES.keys())
        
        # Get decoder mapping info
        decoder_mappings = {}
        for cmd in master_commands + remote_commands + set_commands:
            decoder_mappings[cmd] = CommandDecoderMapping.has_decoder(cmd)
        
        return SupportedCommandsResponse(
            master_commands=master_commands,
            remote_commands=remote_commands,
            set_commands=set_commands,
            total_commands=len(master_commands) + len(remote_commands) + len(set_commands),
            decoder_mappings=decoder_mappings
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get supported commands: {str(e)}"
        )


@app.get("/api/validation/batch-commands/status")
async def get_batch_commands_status():
    """Get batch commands system status and capabilities"""
    return {
        "batch_validation_available": BATCH_VALIDATION_AVAILABLE,
        "decoder_integration": BATCH_VALIDATION_AVAILABLE,
        "supported_modes": ["mock", "live"],
        "supported_command_types": ["master", "remote", "set"],
        "features": {
            "santone_decoder_integration": True,
            "hex_frame_generation": True,
            "timeout_handling": True,
            "detailed_statistics": True,
            "mock_testing": True,
            "live_device_testing": True
        },
        "timestamp": datetime.now().isoformat()
    }


@app.get("/api/results/history")
async def get_results_history(limit: int = 50) -> Dict[str, Any]:
    """Get validation results history"""
    try:
        results = []
        
        if RESULTS_DIR.exists():
            # Get all JSON files sorted by modification time (newest first)
            result_files = sorted(
                RESULTS_DIR.glob("*.json"),
                key=lambda x: x.stat().st_mtime,
                reverse=True
            )[:limit]
            
            for filepath in result_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        data['filename'] = filepath.name
                        results.append(data)
                except Exception as e:
                    logging.warning(f"Error reading result file {filepath}: {e}")
        
        return {
            "status": "success",
            "count": len(results),
            "results": results
        }
        
    except Exception as e:
        logging.error(f"Error getting results history: {e}")
        return {
            "status": "error",
            "message": f"Failed to get results history: {str(e)}",
            "results": []
        }


# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "timestamp": datetime.now().isoformat()
        }
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "src.validation_app:app", 
        host="0.0.0.0", 
        port=8080, 
        reload=True,
        reload_dirs=["src"],
        log_level="info"
    )
